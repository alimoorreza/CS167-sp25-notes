{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimoorreza/CS167-sp25-notes/blob/main/Day19_Building_Simple_MLP_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHJrUI0I4uVJ"
      },
      "source": [
        "# CS167: Day19\n",
        "## Building a Simple MLP using PyTorch Library\n",
        "\n",
        "#### CS167: Machine Learning, Spring 2025\n",
        "\n",
        "\n",
        "ðŸ“œ [Syllabus](https://analytics.drake.edu/~reza/teaching/cs167_sp25/cs167_syllabus_sp25.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XC7VEFe4uVh"
      },
      "source": [
        "# Introduction to PyTorch\n",
        "\n",
        "We can use PyTorch Framework to build and train MLPs and other neural networks such as CNN, RNN, LSTM, Transformers. Let's learn the basics of PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LwgBlHEvJDEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime-->Change runtime type-->GPU or TPU_"
      ],
      "metadata": {
        "id": "OpvuEwrzrVe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if torch.cuda is available, otherwise it will use CPU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "# if it prints 'cuda' then colab is running using GPU device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFa7eTujrik7",
        "outputId": "6ef7d96b-8c83-479b-da51-234d60015510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Building Multilayer Perceptron (MLP)__\n",
        "A multilayer perceptron is the simplest type of neural network. It consists of perceptrons (aka nodes, neurons) arranged in layers.\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_example.png\" width=800/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "euH-4WBP2Gsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate 4 random samples of (x1, x2) for the above network\n",
        "torch.manual_seed(0)                      # for reproducibility\n",
        "random_X = torch.randn(4,2)               # you could imagine that these are pairs of (x1, x2) as shown in the above table\n",
        "print('random_X = \\n', random_X.numpy())\n",
        "\n",
        "\n",
        "input_feature_size = random_X.shape[1]    # number of columns corresponds to feature dimension\n",
        "print('\\n\\ninput feature dimension: ', input_feature_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziMQHP1c72TB",
        "outputId": "ba19c124-7543-479a-ce87-b802ef1ec39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_X = \n",
            " [[ 1.5409961  -0.2934289 ]\n",
            " [-2.1787894   0.56843126]\n",
            " [-1.0845224  -1.3985955 ]\n",
            " [ 0.40334684  0.83802634]]\n",
            "\n",
            "\n",
            "input feature dimension:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can also explicitly incorporate the x0 input which accounts for the bias term in our network\n",
        "# recall that x0 will always be constant value of 1\n",
        "'''\n",
        "input_x_vector = torch.ones(4, 3)\n",
        "input_x_vector[:,1:3] = random_X # using a slicing operation let's squeeze in all (x1, x2) while retaining x0 as 1\n",
        "print(input_x_vector.numpy())\n",
        "input_feature_size = input_x_vector.shape[1] # number of columns corresponds to feature dimension\n",
        "print('\\n\\ninput feature dimension: ', input_feature_size)\n",
        "'''"
      ],
      "metadata": {
        "id": "o5kZ-gcr9Tzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of these questions need to be answered before you set up your neural network:\n",
        "- Q1: how many hidden layers should be there? (depth)\n",
        "- Q2: how many neurons should be in each layer? (width)\n",
        "- Q3:  how many dense connections should be there in between each adjacent layers\n",
        "- Q4: what should the activation be at each of the intermediate layers?\n",
        "  - we could use _sigmoid()_, _tanh()_, _rectified-linear-unit()_, etc\n",
        "- Q5: what should be activation of the final layer\n",
        "  - depends the task _classification_ (sigmoid(), softmax()) vs. _regression_"
      ],
      "metadata": {
        "id": "SsSamDwm3YPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1) # for reproducibility\n",
        "# Q1: how many hidden layers should be there? (depth)\n",
        "# answer: there is only 1 hidden layer\n",
        "num_of_hidden_layer = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Q2: how many neurons should be in each layer? (width)\n",
        "# answer: there are 2 neurons in the input  layer\n",
        "#         there are 3 neurons in the hidden layer\n",
        "#         there are 1 neurons in the output layer\n",
        "#num_of_neurons_input_layer  = input_feature_size # also can be assigned from 'input_feature_size' (which we computed in the previous cell)\n",
        "num_of_neurons_input_layer  = 2\n",
        "num_of_neurons_hidden_layer = 3\n",
        "num_of_neurons_output_layer = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Q3 how many dense connections should be there in between each adjacent layers\n",
        "# answer: there should be 2x3 dense connnections (between input  layer and hidden layer: dense_connections_W1)\n",
        "#         there should be 3x1 dense connnections (between hidden layer and output layer: dense_connections_W2)\n",
        "dense_connections_W1 = torch.randn(num_of_neurons_input_layer,  num_of_neurons_hidden_layer)\n",
        "dense_connections_W2 = torch.randn(num_of_neurons_hidden_layer, num_of_neurons_output_layer)\n",
        "print('Random initialized weights between input  layer and hidden layer: dense_connections_W1=\\n', dense_connections_W1.numpy())\n",
        "print('Random initialized weights between input  layer and hidden layer: dense_connections_W2=\\n', dense_connections_W2.numpy())\n",
        "\n",
        "\n",
        "# add the bias terms for all the layers except input layer\n",
        "bias_terms_hidden    = torch.randn(num_of_neurons_hidden_layer)\n",
        "bias_terms_output    = torch.randn(num_of_neurons_output_layer)\n",
        "print('bias_terms_hidden:\\n', bias_terms_hidden.numpy())\n",
        "print('bias_terms_output:\\n', bias_terms_output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paiHI3yd2y0P",
        "outputId": "d4bbaaaa-8854-4783-9469-dd24a2f215bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random initialized weights between input  layer and hidden layer: dense_connections_W1=\n",
            " [[ 0.66135216  0.2669241   0.06167726]\n",
            " [ 0.6213173  -0.45190597 -0.16613023]]\n",
            "Random initialized weights between input  layer and hidden layer: dense_connections_W2=\n",
            " [[-1.5227685 ]\n",
            " [ 0.38168392]\n",
            " [-1.0276086 ]]\n",
            "bias_terms_hidden:\n",
            " [-0.5630528  -0.89229053 -0.05825018]\n",
            "bias_terms_output:\n",
            " [-0.19550958]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A multilayer perceptron is the simplest type of neural network. It  consists of perceptrons (aka nodes, neurons) arranged in layers. There are 6 connections between input and hidden layer and 3 connections between hidden and output layers with the random initialized using PyTorch code above.\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_example0.png\" width=800/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "MdI_pBInG_Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: what should the activation be at each of the intermediate layers?\n",
        "# answer: let use sigmoid() activation function in the hidden layer\n",
        "sigmoid_activation_hidden = nn.Sigmoid()"
      ],
      "metadata": {
        "id": "iDsEMB1eHBjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: what should be activation of the final layer (let's assume we are using a binary classification task for which sigmoid ctivation is used)\n",
        "sigmoid_activation_output = nn.Sigmoid()"
      ],
      "metadata": {
        "id": "oM-R456_KwYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Forward Pass in Multilayer Perceptron (MLP)__\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_toy_example_forward_pass1.png\" width=800/>\n",
        "</div>\n",
        "\n",
        "Each neuron contains two operations:\n",
        "- a dot product between a weight vector (edges in the graph) and an input vector\n",
        "- that number through an activation function, which produces a number as an output\n",
        "\n",
        "We can collective do all these dot products in a single layer using a single matrix-matrix multiplication [torch.matmul()](https://pytorch.org/docs/stable/generated/torch.matmul.html) as follows.\n",
        "\n",
        "Also add the bias-term after computing the matrix multiplication"
      ],
      "metadata": {
        "id": "52EgSA4RLxyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_mult_X_and_W1 = torch.matmul(random_X[0,:], dense_connections_W1) + bias_terms_hidden\n",
        "print('hidden layer input vector and weight vector dot products: \\n', matrix_mult_X_and_W1.numpy())\n",
        "output_hidden_layer = sigmoid_activation_hidden(matrix_mult_X_and_W1)\n",
        "print('output of hidden layer: \\n', output_hidden_layer.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XrBmU1sN0Gi",
        "outputId": "83adc873-3fc5-44eb-f420-2a267ed5e0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden layer input vector and weight vector dot products: \n",
            " [ 0.27377588 -0.3483593   0.08554165]\n",
            "output of hidden layer: \n",
            " [0.5680196  0.41378036 0.5213724 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_mult_hidden_and_W2 = torch.matmul(output_hidden_layer, dense_connections_W2) + bias_terms_output\n",
        "print('output of output layer: \\n', matrix_mult_hidden_and_W2)\n",
        "final_output = sigmoid_activation_output(matrix_mult_hidden_and_W2)\n",
        "print('output of hidden layer: \\n', final_output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db10EkD0PWyM",
        "outputId": "e43a7a2b-20b3-417a-86c7-27b78dac44b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output of output layer: \n",
            " tensor([-1.4383])\n",
            "output of hidden layer: \n",
            " [0.1918079]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Group activity__#\n",
        "Make another simple MLP with the specifications below and perform the 'Forward Pass' of the MLP."
      ],
      "metadata": {
        "id": "Xsv5W1laMn2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0) # for reproducibility\n",
        "# Q1: how many hidden layers should be there? (depth)\n",
        "# answer: there is only 1 hidden layer\n",
        "num_of_hidden_layer = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Q2: how many neurons should be in each layer? (width)\n",
        "# answer: there are 3 neurons in the input  layer\n",
        "#         there are 4 neurons in the hidden layer\n",
        "#         there are 1 neurons in the output layer\n",
        "num_of_neurons_input_layer  =\n",
        "#num_of_neurons_input_layer  = input_feature_size # also can be assigned from 'input_feature_size' (which we computed in the previous cell)\n",
        "num_of_neurons_hidden_layer =\n",
        "num_of_neurons_output_layer =\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Q3 how many dense connections should be there in between each adjacent layers\n",
        "# answer: there should be ?x? dense connnections (between input  layer and hidden layer: dense_connections_W1)\n",
        "#         there should be ?x1 dense connnections (between hidden layer and output layer: dense_connections_W2)\n",
        "# add the bias terms for all the layers except input layer\n",
        "\n",
        "\n",
        "# Q4: what should the activation be at each of the intermediate layers?\n",
        "# answer: let use sigmoid() activation function in the hidden layer\n",
        "\n",
        "# Q5: what should be activation of the final layer (let's assume we are using a binary classification task for which sigmoid ctivation is used)\n",
        "\n",
        "\n",
        "# do the Forward Pass in Multilayer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "lrnZVVoHReM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Building Modular Code for Multilayer Perceptron (MLP)__\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/mlp_network1.png\" width=800/>\n",
        "</div>\n",
        "\n",
        "A multilayer perceptron is the simplest type of neural network. It consists of perceptrons (aka nodes, neurons) arranged in layers.\n",
        "Create a network class with two methods:\n",
        "- _init()_\n",
        "- _forward()_\n"
      ],
      "metadata": {
        "id": "q4pTsgDsTEiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# You can give any name to your new network, e.g., SimpleMLP.\n",
        "# However, you have to mandatorily inherit from nn.Module to\n",
        "# create your own network class. That way, you can access a lot of\n",
        "# useful methods and attributes from the parent class nn.Module\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # your network layer construction should take place here\n",
        "    # ...\n",
        "    # ...\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for MLP forward pass should take place here\n",
        "    # ...\n",
        "    # ...\n",
        "    return x"
      ],
      "metadata": {
        "id": "9DxGu6AUTW10"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}