{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeC09obpY6xg"
   },
   "source": [
    "# CS167: Day27\n",
    "## Deep Learning and Large Language Models (LLMs)\n",
    "\n",
    "\n",
    "#### CS167: Machine Learning, Spring 2025\n",
    "\n",
    "\n",
    "ðŸ“œ [Syllabus](https://analytics.drake.edu/~reza/teaching/cs167_sp25/cs167_syllabus_sp25.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0CmiA-R3Tk7",
    "outputId": "84294dc8-763c-4103-ace0-e1d5581d244d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRTe6v-v5l9j"
   },
   "source": [
    "#Question Answering Model using Transformer-based large language model (LLM)\n",
    ">**many-to-one** mapping machine learning task:\n",
    "<div>\n",
    "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_sp25/notes/images/many_to_one_ML_task.png?raw=1\" width=200/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325,
     "referenced_widgets": [
      "fe9434d155d14af386cfb20189e08793",
      "4625b1ea30974727ae298e26625c6387",
      "6e155e89056145f4b104e1879bf51fca",
      "ff7db4bd08914b4db79b6a2c47a8800b",
      "693db38f41664c039b3163228eddd392",
      "9c0e4b487eed4ea28fa279ff811fda0a",
      "5e98cd7cd520402d8e14711f16e4dfee",
      "b729b5cc4c2d4aefb4153a798ad6fa95",
      "bc0d19ce851f4db9b4c5664ad7acab81",
      "5ba86242b20a4b4ba20818dec10c2f54",
      "5f14e83cb6a047c59570a9328734ed7c",
      "39b37b0c46a24a26905c856f599c11e6",
      "7981cc5eef0f4bb480e4bda6df43d7ec",
      "fa97560555a24610ac13687e918a62a5",
      "b8f8128e9e9341bebe64a7b30fb662e4",
      "acd55bb6763b4074923b2c741589108f",
      "6be3b46984b14c6686720f186645ab8d",
      "4774a17557174515994195c006d94c93",
      "d3773883e05c47c5b086dd13104a2773",
      "c4bc00b8bae046f7938840c8f3f7ec7f",
      "5b7444676ec641e4a941e7f998cc312b",
      "d1d0c138ae6d4854800a0c6cf177df47",
      "1fafc8598fc74c20b914c5bcbe297e3b",
      "38b46aa3d69c430db61fadc5e12aff58",
      "3ccd5c865e3042cda86351b6efac4e04",
      "f75fa9fac74f4c71a11648292255119b",
      "6d5ed7afb0bd4878a6df6aa9c3e457e1",
      "4aa416483cdd49f5ae7622c3cf4126bd",
      "01bed7f15d6f4fc0a8b7fdc54ddc97d0",
      "69616126220044d1a0342837689c44ba",
      "f2142f74721c44bdbef57ee2f214fd7c",
      "8c3995ebcf3f4d3f8af2fa100cd18b71",
      "453aab95affa4a60b0c51ac7c7413d33",
      "434a97beee504584920a1b2557915f41",
      "ee978961f5eb4d6bbc8a997a69ded323",
      "5bfdf4b6cb2a47f3a87ddb4816db9b44",
      "d5cc4521136f4915b21c57854f165fbc",
      "e84fb9bf16d24f9487dca30c7913099e",
      "c9c818624d4f492dabfdd5d86b7074c9",
      "790431b161db45878eca7905932d15fe",
      "cc97ddf29d034485be3e9df1927277fb",
      "cbd2a8e221e54b72a52c095341fe7614",
      "213b96575eee417ab07adc5eb6ca7a38",
      "f4235b1167e2476e8cf098e21fe0fb95",
      "88d30fcc65fe46c68d47118d627d7bb0",
      "fc6f1de9d802434b9439fd6607f9a35a",
      "618706090e5143ab8c3e846bde367d17",
      "dda94543818c49ee943b32fe6ff530f8",
      "c20a3ca7c05c42f5946c369c6865fad8",
      "deb9d0a3efe14b6ca642976c33d88e10",
      "42cc03514d3c4159af88a57e2bbbcc9a",
      "64756cf2bef543faa6f76bd11086a907",
      "8e77139b5b9c4e5a84ca3436fdd0569a",
      "2ed51d9b2ea0426bb56178bd1e8ba7dd",
      "470b22c5b3e84a87b6ac3eb818dced26"
     ]
    },
    "id": "9Hc5LKqp36rt",
    "outputId": "38129e2d-ee8c-4e89-da97-7433cb96990a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9434d155d14af386cfb20189e08793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b37b0c46a24a26905c856f599c11e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fafc8598fc74c20b914c5bcbe297e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434a97beee504584920a1b2557915f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d30fcc65fe46c68d47118d627d7bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa_nlp_model = pipeline(\"question-answering\") # Uses DistilBERT model by default (on 05/06/24 in Reza's machine) distilbert/distilbert-base-cased-distilled-squad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gvds0x544JKV",
    "outputId": "da13f1d9-c18c-49a6-977b-594aae0c8487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'an iconic ivory-white marble mausoleum\n",
      "Answer: 'Mughal emperor Shah Jahan\n",
      "Answer: 'Mughal architecture\n"
     ]
    }
   ],
   "source": [
    "context = r\"\"\"The Taj Mahal is an iconic ivory-white marble mausoleum located in Agra, India. ...\n",
    "           It was commissioned in 1631 by the Mughal emperor Shah Jahan to serve as the tomb for his beloved wife Mumtaz Mahal. ...\n",
    "           The mausoleum is the centerpiece of a large complex that includes a mosque, guest house, and formal gardens surrounded by walls. ...\n",
    "           The Taj Mahal is considered a masterpiece of Mughal architecture and is one of the most famous buildings in the world. ...\"\"\"\n",
    "\n",
    "question_sentences = [\"What is Taj Mahal?\", \\\n",
    "                      \"Who commissioned Taj Mahal?\", \\\n",
    "                      \"What Taj Mahal is famous for?\"]\n",
    "\n",
    "for ii in range(len(question_sentences)):\n",
    "  result = qa_nlp_model(question=question_sentences[ii], context=context)\n",
    "\n",
    "  verbose = 1\n",
    "  if verbose:\n",
    "    print(f\"Answer: '{result['answer']}\")\n",
    "  else:\n",
    "    print(f\"Answer: '{result['answer']}, score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJEaDuPhD_1_"
   },
   "source": [
    "Let's pick a particular pre-trained model. To ensure we can easily load different models using the same PyTorch commands, we can use this [AutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForQuestionAnswering) module.Let's try first __BERT (Bidirectional Encoder Representations from Transformers)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65IXtkQ28UNV",
    "outputId": "01bda5f6-6871-4459-c4c5-6fb6daf6b896"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/longformer-base-4096-finetuned-squadv2 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Taj Mahal?\n",
      "Answer:  an iconic ivory-white marble mausoleum\n",
      "Question: Who commissioned Taj Mahal?\n",
      "Answer:  Mughal emperor Shah Jahan\n",
      "Question: What Taj Mahal is famous for?\n",
      "Answer:  masterpiece of Mughal architecture\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "#model_name            = \"bert-large-uncased-whole-word-masking-finetuned-squad\"  # BERT\n",
    "model_name           = \"mrm8488/longformer-base-4096-finetuned-squadv2\"\n",
    "#model_name           = \"mrm8488/longformer-base-4096-finetuned-squadv2\"\n",
    "\n",
    "tokenizer             = AutoTokenizer.from_pretrained(model_name)\n",
    "model                 = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "context = r\"\"\"The Taj Mahal is an iconic ivory-white marble mausoleum located in Agra, India. ...\n",
    "           It was commissioned in 1631 by the Mughal emperor Shah Jahan to serve as the tomb for his beloved wife Mumtaz Mahal. ...\n",
    "           The mausoleum is the centerpiece of a large complex that includes a mosque, guest house, and formal gardens surrounded by walls. ...\n",
    "           The Taj Mahal is considered a masterpiece of Mughal architecture and is one of the most famous buildings in the world. ...\"\"\"\n",
    "\n",
    "question_sentences = [\"What is Taj Mahal?\", \\\n",
    "                      \"Who commissioned Taj Mahal?\", \\\n",
    "                      \"What Taj Mahal is famous for?\"]\n",
    "\n",
    "\n",
    "\n",
    "for question in question_sentences:\n",
    "  inputs              = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "  input_ids           = inputs[\"input_ids\"].tolist()[0]\n",
    "  #text_tokens        = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "  output              = model(**inputs)\n",
    "  answer_start_scores = output['start_logits'].detach()\n",
    "  answer_end_scores   = output['end_logits'].detach()\n",
    "\n",
    "  answer_start        = torch.argmax(answer_start_scores)    # get the most likely beginning of answer with the argmax of the score\n",
    "  answer_end          = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "  answer              = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "  print(f\"Question: {question}\")\n",
    "  print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9RlgTGBGNbG"
   },
   "source": [
    "BERT is just one of the many LLM pretrained models for quetion answering. [Huggingface hosts a repository of these pretrained models](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForQuestionAnswering). There several others:\n",
    "\n",
    "\n",
    "*   [LongFormer](https://huggingface.co/mrm8488/longformer-base-4096-finetuned-squadv2)\n",
    "*   [DistilBERT](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)\n",
    "*   [RoBERTa](https://huggingface.co/deepset/roberta-base-squad2)\n",
    "*   [ALBERT](https://huggingface.co/twmkn9/albert-base-v2-squad2)\n",
    "*   [XLM RoBERTa](https://huggingface.co/TunahanGokcimen/Question-Answering-xlm-roberta-base)\n",
    "*   [MobileBERT](https://huggingface.co/csarron/mobilebert-uncased-squad-v2)                    \n",
    "*   [XLNet](https://huggingface.co/xlnet/xlnet-base-cased)\n",
    "*   [T5](https://huggingface.co/sjrhuschlee/flan-t5-base-squad2)\n",
    "*   [GPT2](https://huggingface.co/openai-community/gpt2)\n",
    "\n",
    "\n",
    "There are a few more recent extra-large models (more than 8GB-10GB) such as BLOOM and Llama. You could try those later if you want.\n",
    "*   [BLOOM](https://huggingface.co/bigscience/bloom)\n",
    "*   [Llama2](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat)\n",
    "\n",
    "# __Group Activty#3__\n",
    "> 1. Try another pretrained model __RoBERTa, LongFormer, ALBERT, XLM RoBERTa, MobileBERT, T5, GPT2__ and redo the Question-Answering task.\n",
    "> 2. Try out your pre-trained model by asking it some additional questions based on the provided information.\n",
    "> 3. Now, expand the context by adding a few more sentences and check if the model can still answer your questions accurately.\n",
    "> 4. Find the size (in megabytes)of each pretrained model to gain insight into its scale and magnitude.\n",
    "> 5. Examine and document the variations in responses generated by distinct pre-trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnt-OxYFGnbl",
    "outputId": "a204d02a-5150-4f0c-b68c-d7cfe99d4235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Taj Mahal?\n",
      "Answer: an iconic ivory - white marble mausoleum\n",
      "Question: Who commissioned Taj Mahal?\n",
      "Answer: shah jahan\n",
      "Question: What Taj Mahal is famous for?\n",
      "Answer: mughal architecture and is one of the most famous buildings in the world\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq8aVhT2U8S7"
   },
   "source": [
    "So far, we have only worked with a pre-trained model, which worked just fine. If you want to create your own pretrained model using your own Question Answering dataset, you can fine-tune a transformer model.\n",
    "\n",
    "For fine-tuning a Question Answering Model, curious students can explore this Notebook from Huggingface further.\n",
    "> [Question Answering model fine-tuning](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
